{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet RAG (Retrieval Augmented Generation) avec LangChain et Google Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook implémente un système de Retrieval Augmented Generation (RAG) pour interagir avec des documents PDF en utilisant les capacités des modèles de langage de Google Gemini via le framework LangChain.\n",
    "L'objectif principal de ce projet est de construire une application qui peut :\n",
    "Extraire du texte à partir d'un document PDF, dans ce cas le document choisi c'est le \"AI ACT\" de la UE.\n",
    "Diviser ce texte en petits morceaux (chunks).\n",
    "Vectoriser ces chunks pour les rendre interrogeables sémantiquement.\n",
    "Récupérer dynamiquement les informations les plus pertinentes du PDF en réponse à une question posée par l'utilisateur.\n",
    "Utiliser un modèle de langage (LLM) pour générer une réponse concise et précise basée uniquement sur les informations récupérées, évitant ainsi les \"hallucinations\" et assurant la pertinence contextuelle.\n",
    "Le notebook est divisé en plusieurs blocs de code, chacun ayant un rôle spécifique dans la chaîne de traitement RAG. Des messages d'avancement sont inclus pour suivre la progression étape par étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des bibliothèques nécessaires\n",
    "!pip install PyMuPDF langchain langchain-community langchain-google-genai google-generativeai chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloc 1 : Initialisation (Imports, API, Modèles)\n",
    "Objectif : Préparer l'environnement en important toutes les bibliothèques Python nécessaires et en initialisant les composants clés de notre système d'IA. Cela inclut la configuration de votre clé API Google Gemini et l'instanciation du modèle de langage (LLM) pour la génération de texte (gemini-2.0-flash) et du modèle d'embeddings pour la vectorisation du texte (models/embedding-001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bloc 1 : Initialisation ---\n",
      "Modules importés et modèles LLM/Embeddings initialisés avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Imports nécessaires\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import google.generativeai as genai\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Nouveaux imports pour RAG avec LangChain\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "print(\"--- Bloc 1 : Initialisation ---\")\n",
    "\n",
    "# Initialiser le client Gemini et la clé API\n",
    "api_key = \"AIzaSyCEmBeUR9t06kZcwYogQZc_NRrUH6fH67c\" # Remplacez par votre vraie clé API si ce n'est pas un exemple\n",
    "genai.configure(api_key=api_key) # Configurer le client genai globalement\n",
    "\n",
    "# Utiliser le wrapper LangChain pour Gemini LLM (modèle de chat)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=api_key)\n",
    "\n",
    "# Initialiser le modèle d'embeddings pour la recherche vectorielle\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\n",
    "\n",
    "print(\"Modules importés et modèles LLM/Embeddings initialisés avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloc 2 : Chargement du PDF et Extraction du Texte\n",
    "Objectif : Lire le document PDF spécifié et en extraire tout le contenu textuel brut. Cette étape est cruciale car elle transforme le document structuré en une chaîne de caractères utilisable par les étapes suivantes du pipeline RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bloc 2 : Chargement du PDF et Extraction du Texte ---\n",
      "Texte extrait avec succès du PDF 'C:/Users/Mike0/Downloads/aiact_final_draft.pdf/aiact_final_draft.pdf'.\n",
      "Longueur totale du texte : 619242 caractères.\n",
      "Début du texte extrait (premiers 5000 caractères) :\n",
      "---\n",
      "  \n",
      "5662/24  \n",
      " \n",
      "RB/ek \n",
      "1\n",
      "TREE.2.B \n",
      "LIMITE \n",
      "EN\n",
      " \n",
      "Council of the \n",
      "European Union \n",
      "Brussels, 26 January 2024 \n",
      "(OR. en) \n",
      "5662/24 \n",
      "LIMITE \n",
      "TELECOM 22 \n",
      "JAI 98 \n",
      "COPEN 18 \n",
      "CYBER 14 \n",
      "DATAPROTECT 32 \n",
      "EJUSTICE 3 \n",
      "COSI 6 \n",
      "IXIM 15 \n",
      "ENFOPOL 21 \n",
      "RELEX 77 \n",
      "MI 65 \n",
      "COMPET 68 \n",
      "CODEC 133 \n",
      "Interinstitutional File: \n",
      "2021/0106(COD) \n",
      " \n",
      " \n",
      "NOTE \n",
      "From: \n",
      "Presidency \n",
      "To: \n",
      "Permanent Representatives Committee \n",
      "No. Cion doc.: \n",
      "8115/21 \n",
      "Subject: \n",
      "Proposal for a Regulation of the European Parliament and of the Council \n",
      "laying down harmonised rules on artificial intelligence (Artificial Intelligence \n",
      "Act)  and amending certain Union legislative acts \n",
      "- Analysis of the final compromise text with a view to agreement \n",
      " \n",
      " \n",
      "I. \n",
      "INTRODUCTION \n",
      "1. \n",
      "The Commission adopted the proposal for a Regulation laying down harmonised rules on \n",
      "artificial intelligence (Artificial Intelligence Act, hereinafter: the AI Act) on 21 April 2021. \n",
      " \n",
      "2. \n",
      "The Council unanimously adopted its General Approach on the proposal on 6 December \n",
      "2022, while the European Parliament (hereinafter: the EP) confirmed its position in a plenary \n",
      "vote on 14 June 2023. \n",
      " \n",
      "3. \n",
      "On 14 June 2023, 18 July 2023, 2-3 October 2023 and 24 October 2023 the first four political \n",
      "trilogues were held, during which some of the less controversial parts of the proposal were \n",
      "agreed and compromise was also found on the provisions concerning measures in support of \n",
      " \n",
      "5662/24  \n",
      " \n",
      "RB/ek \n",
      "2\n",
      "TREE.2.B \n",
      "LIMITE \n",
      "EN\n",
      " \n",
      "innovation, as well as and on the mechanism for classification of AI systems as high-risk. \n",
      "Moreover, during those initial trilogues the co-legislators explored potential landing zones \n",
      "with regard to the remaining issues, in particular the regulation of general purpose AI models \n",
      "and systems, governance, as well the prohibitions and law enforcement package. \n",
      " \n",
      "4. \n",
      "On 29 November 2023 and 1 December 2023, the Committee of Permanent Representatives \n",
      "granted the Presidency a revised mandate to continue the negotiations on all the outstanding \n",
      "elements of the proposal. The fifth and final trilogue was held between 6 and 8 December \n",
      "2023. During this extended round of negotiations the Council and the EP came to an \n",
      "agreement on all political issues and successfully closed the interinstitutional negotiations. \n",
      "Following this political agreement, the co-legislators proceeded to carry out further technical \n",
      "work in January 2024 in order to align the text of the recitals with the text of the articles as \n",
      "agreed during the final trilogue.  \n",
      " \n",
      "5. \n",
      "In the Annex to this document delegations will find the text of the proposal for a Regulation \n",
      "laying down harmonised rules on artificial intelligence (the AI Act), updated according to the \n",
      "provisional political agreement reached at the fifth trilogue between 6 and 8 December 2023. \n",
      " \n",
      "II. \n",
      "MAIN ELEMENTS OF THE COMPROMISE \n",
      "1. Subject matter and scope  \n",
      " \n",
      "As regards the subject matter, the compromise text of Article 1(1) includes a high-level \n",
      "statement that one of the purposes of the AI Act is to ensure a high level of protection of \n",
      "health, safety and fundamental rights enshrined in the Charter, which includes democracy, \n",
      "rule of law and environmental protection. However, all subsequent references in the text to \n",
      "the risks addressed by the Regulation only include risks to health, safety and fundamental \n",
      "rights, in line with the Council’s mandate. \n",
      " \n",
      "Concerning the scope, the compromise text makes it clear that national security is excluded. \n",
      "The wording concerning this exclusion in Article 2(3) has been fine tuned to align it more \n",
      "closely with the respective language used in recently agreed legal acts, such as the Cyber \n",
      "Resilience Act and the Data Act, while the text of the corresponding Recital 12a has \n",
      "remained the same as in the Council’s mandate.  \n",
      " \n",
      " \n",
      "5662/24  \n",
      " \n",
      "RB/ek \n",
      "3\n",
      "TREE.2.B \n",
      "LIMITE \n",
      "EN\n",
      " \n",
      "2. Definition of an AI system \n",
      " \n",
      "The definition of an AI system in Article 3(1) has been modified to align it more closely \n",
      "with the work of international organisations working on artificial intelligence, notably the \n",
      "OECD.  Moreover, the corresponding Recital 6 details further the key characteristics of the \n",
      "definition and clarifies that the definition is not intended to cover simpler traditional \n",
      "software systems or programming approaches, which are based on the rules defined solely \n",
      "by natural persons to automatically execute operations. Additionally, the Commission has \n",
      "been tasked to develop guidelines on the application of the definition of an AI system.  \n",
      " \n",
      "3. Prohibited AI practices \n",
      " \n",
      "Regarding the list of prohibited AI practices in Article 5, the compromise text includes the \n",
      "changes as previously accepted by the Committee of Permanent Representatives in the \n",
      "Council’s revised mandate. This means that the list includes the prohibition of real-time \n",
      "biometric identification by law enforcement authorities in publicly accessible spaces, but \n",
      "with some notable and clearly defined exceptions, as listed in Article 5(1)(d), which are \n",
      "now subject to a range of safe...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Bloc 2 : Chargement du PDF et Extraction du Texte ---\")\n",
    "\n",
    "# Chemin vers le PDF\n",
    "pdf_document = \"C:/Users/Mike0/Downloads/aiact_final_draft.pdf/aiact_final_draft.pdf\"\n",
    "\n",
    "try:\n",
    "    doc = fitz.open(pdf_document)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    doc.close() # Fermer le document après extraction\n",
    "\n",
    "    print(f\"Texte extrait avec succès du PDF '{pdf_document}'.\")\n",
    "    print(f\"Longueur totale du texte : {len(text)} caractères.\")\n",
    "    print(f\"Début du texte extrait (premiers 5000 caractères) :\\n---\\n{text[:5000]}...\\n---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'extraction du texte du PDF : {e}\")\n",
    "    text = \"\" # S'assurer que 'text' est défini même en cas d'erreur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloc 3 : Division du Texte en Chunks\n",
    "Objectif : Découper le long texte extrait du PDF en morceaux plus petits, appelés \"chunks\". Cette division est essentielle car les modèles d'embeddings et les LLM ont des limites de taille de contexte. Un chevauchement est également défini pour s'assurer qu'aucun contexte important ne soit perdu aux frontières des chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bloc 3 : Division du Texte en Chunks ---\n",
      "Texte divisé en 784 chunks.\n",
      "Exemple du premier chunk (longueur 984) :\n",
      "---\n",
      "5662/24  \n",
      " \n",
      "RB/ek \n",
      "1\n",
      "TREE.2.B \n",
      "LIMITE \n",
      "EN\n",
      " \n",
      "Council of the \n",
      "European Union \n",
      "Brussels, 26 January 2024 \n",
      "(OR. en) \n",
      "5662/24 \n",
      "LIMITE \n",
      "TELECOM 22 \n",
      "JAI 98 \n",
      "COPEN 18 \n",
      "CYBER 14 \n",
      "DATAPROTECT 32 \n",
      "EJUSTICE 3 \n",
      "COSI 6 \n",
      "IXIM 15 \n",
      "ENFOPOL 21 \n",
      "RELEX 77 \n",
      "MI 65 \n",
      "COMPET 68 \n",
      "CODEC 133 \n",
      "Interinstitutional File: \n",
      "2021/0106(COD) \n",
      " \n",
      " \n",
      "NOTE \n",
      "From: \n",
      "Presidency \n",
      "To: \n",
      "Permanent Representatives Committee \n",
      "No. Cion doc.: \n",
      "8115/21 \n",
      "Subject: \n",
      "Proposal for a Regulation of the European Parliament and of the Council \n",
      "laying down harmonised rules on artificial intelligence (Artificial Intelligence \n",
      "Act)  and amending certain Union legislative acts \n",
      "- Analysis of the final compromise text with a view to agreement \n",
      " \n",
      " \n",
      "I. \n",
      "INTRODUCTION \n",
      "1. \n",
      "The Commission adopted the proposal for a Regulation laying down harmonised rules on \n",
      "artificial intelligence (Artificial Intelligence Act, hereinafter: the AI Act) on 21 April 2021. \n",
      " \n",
      "2. \n",
      "The Council unanimously adopted its General Approach on the proposal on 6 December...\n",
      "---\n",
      "Exemple du deuxième chunk (longueur 995) :\n",
      "---\n",
      "artificial intelligence (Artificial Intelligence Act, hereinafter: the AI Act) on 21 April 2021. \n",
      " \n",
      "2. \n",
      "The Council unanimously adopted its General Approach on the proposal on 6 December \n",
      "2022, while the European Parliament (hereinafter: the EP) confirmed its position in a plenary \n",
      "vote on 14 June 2023. \n",
      " \n",
      "3. \n",
      "On 14 June 2023, 18 July 2023, 2-3 October 2023 and 24 October 2023 the first four political \n",
      "trilogues were held, during which some of the less controversial parts of the proposal were \n",
      "agreed and compromise was also found on the provisions concerning measures in support of \n",
      " \n",
      "5662/24  \n",
      " \n",
      "RB/ek \n",
      "2\n",
      "TREE.2.B \n",
      "LIMITE \n",
      "EN\n",
      " \n",
      "innovation, as well as and on the mechanism for classification of AI systems as high-risk. \n",
      "Moreover, during those initial trilogues the co-legislators explored potential landing zones \n",
      "with regard to the remaining issues, in particular the regulation of general purpose AI models \n",
      "and systems, governance, as well the prohibitions and law enforcement package....\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Bloc 3 : Division du Texte en Chunks ---\")\n",
    "\n",
    "# Diviser le texte en chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Nombre de caractères par chunk\n",
    "    chunk_overlap=200  # Chevauchement entre les chunks\n",
    ")\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "print(f\"Texte divisé en {len(chunks)} chunks.\")\n",
    "if chunks:\n",
    "    print(f\"Exemple du premier chunk (longueur {len(chunks[0])}) :\\n---\\n{chunks[0]}...\\n---\")\n",
    "    if len(chunks) > 1:\n",
    "        print(f\"Exemple du deuxième chunk (longueur {len(chunks[1])}) :\\n---\\n{chunks[1]}...\\n---\")\n",
    "else:\n",
    "    print(\"Aucun chunk n'a été créé. Vérifiez si le texte du PDF n'était pas vide.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloc 4 : Vectorisation et Stockage Vectoriel\n",
    "Objectif : Convertir chaque chunk de texte en une représentation numérique dense (un \"embedding\" ou \"vecteur\"). Ces vecteurs capturent le sens sémantique du texte. Ensuite, ces embeddings sont stockés dans une base de données vectorielle (ici, ChromaDB en mémoire). C'est cette base de données qui permettra une recherche rapide et sémantiquement pertinente des informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bloc 4 : Vectorisation et Stockage Vectoriel ---\n",
      "Base de données vectorielle Chroma créée avec succès. 784 documents indexés.\n",
      "Les données sont maintenant vectorisées et prêtes pour la recherche sémantique.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Bloc 4 : Vectorisation et Stockage Vectoriel ---\")\n",
    "\n",
    "# Convertir les chunks en objets Document de LangChain\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "if documents:\n",
    "    # Créer la base de données vectorielle à partir des documents et du modèle d'embeddings\n",
    "    # C'est ici que les embeddings sont générés et stockés (en mémoire dans ce cas avec Chroma)\n",
    "    vectorstore = Chroma.from_documents(documents, embeddings_model)\n",
    "    print(f\"Base de données vectorielle Chroma créée avec succès. {len(documents)} documents indexés.\")\n",
    "    print(\"Les données sont maintenant vectorisées et prêtes pour la recherche sémantique.\")\n",
    "else:\n",
    "    print(\"Aucun document à vectoriser. La base de données vectorielle n'a pas été créée.\")\n",
    "    vectorstore = None # S'assurer que vectorstore est None si aucun document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloc 5 : Configuration de la Chaîne RAG\n",
    "Objectif : Assembler les différents composants (le retriever, le modèle de langage et le prompt) pour créer la chaîne RAG complète. Le \"retriever\" est configuré pour rechercher les chunks les plus pertinents dans la base de données vectorielle. Le \"prompt\" est conçu pour guider le LLM à utiliser le contexte récupéré pour générer sa réponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bloc 5 : Configuration de la Chaîne RAG ---\n",
      "Chaîne RAG configurée avec succès (Retriever et Generation).\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Bloc 5 : Configuration de la Chaîne RAG ---\")\n",
    "\n",
    "if vectorstore:\n",
    "    # Créer un retriever à partir de la base de données vectorielle\n",
    "    # search_kwargs={\"k\": 3} signifie qu'il récupérera les 3 chunks les plus pertinents\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    # Définir le prompt pour la combinaison des documents\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Répondez à la question de l'utilisateur en vous basant uniquement sur le contexte fourni ci-dessous:\\n\\n{context}\"),\n",
    "        MessagesPlaceholder(\"history\"), # Pour l'historique de conversation si on en ajoute plus tard\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    # Créer la chaîne pour combiner les documents avec le LLM\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    # Créer la chaîne de récupération complète (RAG)\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    print(\"Chaîne RAG configurée avec succès (Retriever et Generation).\")\n",
    "else:\n",
    "    print(\"Impossible de configurer la chaîne RAG car la base de données vectorielle n'a pas été créée.\")\n",
    "    retrieval_chain = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloc 6 : Pose de la Question et Obtention de la Réponse (avec Contexte)\n",
    "Objectif : Exécuter le système RAG. Une question est soumise à la chaîne. La chaîne récupère automatiquement les chunks les plus pertinents du PDF, les passe au LLM comme contexte, et le LLM génère une réponse basée sur ces informations. Ce bloc affiche non seulement la réponse finale, mais aussi les chunks spécifiques qui ont été utilisés par le modèle pour formuler cette réponse, offrant ainsi une transparence sur le processus de \"retrieval\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bloc 6 : Pose de la Question et Obtention de la Réponse ---\n",
      "Question posée : 'what is the definition of an AI system ?'\n",
      "\n",
      "--- Réponse de l'agent RAG ---\n",
      "AI systems are characterized by their capability to infer, which involves obtaining outputs like predictions, content, recommendations, or decisions that can influence physical and virtual environments. This capability includes deriving models and/or algorithms from inputs/data. The techniques that enable inference while building an AI system include machine learning approaches that learn from data how to achieve certain objectives; and logic- and knowledge-based approaches that infer from encoded knowledge or symbolic representation of the task to be solved. The capacity of an AI system to infer goes beyond basic data processing, enable learning, reasoning or modelling. The term “machine-based” refers to the fact that AI systems run on machines.\n",
      "\n",
      "--- Chunks de document utilisés pour cette réponse (les 3 plus pertinents) ---\n",
      "\n",
      "Chunk 1 (longueur 978 caractères) :\n",
      "------------------------------------------------------------------\n",
      "should not cover systems that are based on the rules defined solely by natural persons to \n",
      "automatically execute operations. A key characteristic of AI systems is their capability to \n",
      "infer. This inference refers to the process of obtaining the outputs, such as predictions, \n",
      "content, recommendations, or decisions, which can influence physical and virtual \n",
      "environments and to a capability of AI systems to derive models and/or algorithms from \n",
      "inputs/data. The techniques that enable inference while building an AI system include \n",
      "machine learning approaches that learn from data how to achieve certain objectives; and \n",
      "logic- and knowledge-based approaches that infer from encoded knowledge or symbolic \n",
      "representation of the task to be solved. The capacity of an AI system to infer goes beyond \n",
      "basic data processing, enable learning, reasoning or modelling. The term “machine-based” \n",
      "refers to the fact that AI systems run on machines. The reference to explicit or implicit\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Chunk 2 (longueur 978 caractères) :\n",
      "------------------------------------------------------------------\n",
      "should not cover systems that are based on the rules defined solely by natural persons to \n",
      "automatically execute operations. A key characteristic of AI systems is their capability to \n",
      "infer. This inference refers to the process of obtaining the outputs, such as predictions, \n",
      "content, recommendations, or decisions, which can influence physical and virtual \n",
      "environments and to a capability of AI systems to derive models and/or algorithms from \n",
      "inputs/data. The techniques that enable inference while building an AI system include \n",
      "machine learning approaches that learn from data how to achieve certain objectives; and \n",
      "logic- and knowledge-based approaches that infer from encoded knowledge or symbolic \n",
      "representation of the task to be solved. The capacity of an AI system to infer goes beyond \n",
      "basic data processing, enable learning, reasoning or modelling. The term “machine-based” \n",
      "refers to the fact that AI systems run on machines. The reference to explicit or implicit\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Chunk 3 (longueur 918 caractères) :\n",
      "------------------------------------------------------------------\n",
      "notion of AI systems to enable legal certainty. The definition should be based on the key \n",
      "functional characteristics of a general-purpose AI model, in particular the generality and \n",
      "the capability to competently perform a wide range of distinct tasks. These models are \n",
      "typically trained on large amounts of data, through various methods, such as self-\n",
      "supervised, unsupervised or reinforcement learning. General purpose AI models may be \n",
      "placed on the market in various ways, including through libraries, application \n",
      "programming interfaces (APIs), as direct download, or as physical copy. These models \n",
      "may be further modified or fine-tuned into new models. Although AI models are essential \n",
      "components of AI systems, they do not constitute AI systems on their own. AI models \n",
      " \n",
      "5662/24  \n",
      " \n",
      "RB/ek \n",
      "59\n",
      "TREE.2.B \n",
      "LIMITE \n",
      "EN\n",
      " \n",
      "require the addition of further components, such as for example a user interface, to become\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Bloc 6 : Pose de la Question et Obtention de la Réponse ---\")\n",
    "\n",
    "if retrieval_chain:\n",
    "    # Exemple d'utilisation\n",
    "    question = \"what is the definition of an AI system ?\"\n",
    "    print(f\"Question posée : '{question}'\")\n",
    "\n",
    "    # Appeler la chaîne de récupération avec la question\n",
    "    # Le processus de recherche des chunks pertinents et d'envoi au LLM est géré automatiquement\n",
    "    response = retrieval_chain.invoke({\"input\": question, \"history\": []})\n",
    "\n",
    "    print(\"\\n--- Réponse de l'agent RAG ---\")\n",
    "    print(response[\"answer\"])\n",
    "\n",
    "    print(\"\\n--- Chunks de document utilisés pour cette réponse (les 3 plus pertinents) ---\")\n",
    "    for i, doc in enumerate(response[\"context\"]):\n",
    "        print(f\"\\nChunk {i+1} (longueur {len(doc.page_content)} caractères) :\")\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "        print(doc.page_content)\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "else:\n",
    "    print(\"La chaîne RAG n'est pas prête pour répondre aux questions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
